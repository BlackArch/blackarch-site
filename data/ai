adversarial-robustness-toolbox|1.20.1.r155.g23539e2|Python Library for Machine Learning Security.|https://github.com/Trusted-AI/adversarial-robustness-toolbox
cai|0.5.9.r18.ge22a122|The framework for AI Security.|https://github.com/aliasrobotics/cai
cleverhans|v4.0.0.r7.g574efc1|Python library to benchmark machine learning systems vulnerability to adversarial examples.|https://github.com/cleverhans-lab/cleverhans
promptfoo|0.120.20|Test and evaluate LLM outputs - AI red teaming, pentesting, and vulnerability scanning.|https://github.com/promptfoo/promptfoo
